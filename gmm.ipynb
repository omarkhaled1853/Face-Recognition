{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c0cb43cf-5102-405b-bea7-2183ffe77f75",
      "cell_type": "code",
      "source": "import zipfile\nimport os\n\nzip_path = \"archive.zip\"\n\nextract_dir = \"att_faces\"\nos.makedirs(extract_dir, exist_ok=True)\n\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_dir)\n\nprint(f\"Dataset extracted to: {os.path.abspath(extract_dir)}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Dataset extracted to: /drive/notebooks/att_faces\n"
        }
      ],
      "execution_count": 11
    },
    {
      "id": "10a58268-773b-42ca-8dbe-30b27b7685cf",
      "cell_type": "code",
      "source": "from PIL import Image\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\ndata_matrix = np.zeros((400, 10304))\nrow_index = 0\n\ndataset_path = \"att_faces\"\n\nfor subject in range(1, 41):\n    subject_dir = os.path.join(dataset_path, f\"s{subject}\")\n\n    for img_num in range(1, 11):\n        img_path = os.path.join(subject_dir, f\"{img_num}.pgm\")\n\n        img = Image.open(img_path)\n        img_array = np.array(img).flatten()\n\n        data_matrix[row_index, :] = img_array\n        row_index += 1\n\nprint(data_matrix.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "(400, 10304)\n"
        }
      ],
      "execution_count": 12
    },
    {
      "id": "965d2856-98e6-44b2-9bd2-6f22dcdef128",
      "cell_type": "code",
      "source": "y = np.zeros(400, dtype=int)\n\nfor i in range (40):\n    y[i * 10 : (i + 1) * 10] = i + 1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "id": "44c5e22e-fe39-4b2d-a07a-c58cf94234f6",
      "cell_type": "code",
      "source": "X_train = data_matrix[::2]\ny_train = y[::2]\n\nX_test = data_matrix[1::2]\ny_test = y[1::2]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "id": "5a0f7209-6a31-497d-af13-d2ef9184aa50",
      "cell_type": "code",
      "source": "import os\nfor f in ['eigenvalues.npy', 'eigenvectors.npy']:\n    if os.path.exists(f):\n        os.remove(f)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "id": "e50b9239-7a13-4a2a-b7b9-c88b7fadb0a2",
      "cell_type": "code",
      "source": "class PCA:\n    def __init__(self, n_components=None, alpha=None):\n        \"\"\"\n        Parameters:\n        - n_components: int, number of components to keep\n        - alpha: float (0-1), minimum variance ratio to retain\n        \"\"\"\n        self.n_components = n_components\n        self.alpha = alpha\n        self.components = None\n        self.mean = None\n        self.explained_variance_ratio = None\n    \n    def fit(self, X):\n        # 1. Standardize the data (center to mean)\n        self.mean = np.mean(X, axis=0)\n        X_centered = X - self.mean\n        \n        # Save/load eigenvalues and eigenvectors\n        if os.path.exists('eigenvalues.npy') and os.path.exists('eigenvectors.npy'):\n            self.eigenvalues = np.load('eigenvalues.npy')\n            self.eigenvectors = np.load('eigenvectors.npy')\n        else:\n            cov_matrix = np.cov(X_centered, rowvar=False)\n            eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n            sorted_indices = np.argsort(eigenvalues)[::-1]\n            self.eigenvalues = eigenvalues[sorted_indices]\n            self.eigenvectors = eigenvectors[:, sorted_indices]\n            \n            np.save('eigenvalues.npy', self.eigenvalues)\n            np.save('eigenvectors.npy', self.eigenvectors)\n        \n        # 5. Compute explained variance ratio ??\n        total_variance = np.sum(self.eigenvalues)\n        self.explained_variance_ratio = self.eigenvalues / total_variance\n        \n        # 6. Determine number of components to keep \n        if self.alpha is not None:\n            cumulative_variance = np.cumsum(self.explained_variance_ratio)\n            self.n_components = np.argmax(cumulative_variance >= self.alpha) + 1\n        \n        # 7. Select top components with max eigenvalue\n        self.components = self.eigenvectors[:, :self.n_components] ## using in inverse transform\n        \n        return self\n    \n    def transform(self, X):\n        # Center the data using the mean from training\n        X_centered = X - self.mean\n        \n        # Project data onto principal components\n        return np.dot(X_centered, self.components)\n    \n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "id": "25439db7-43b4-48cf-aaf6-7847841d3e32",
      "cell_type": "code",
      "source": "def apply_pca(X_train, X_test, alpha=0.9): ## default with 99%\n    \"\"\"\"\n    make a pojection for x_test  not need to fit but train must fit then make a projuction\n    make a fit transform for x_train \n\n    \"\"\"\n    pca = PCA(alpha=alpha)  \n    X_train_pca = pca.fit_transform(X_train)  \n    X_test_pca = pca.transform(X_test) \n    return X_train_pca, X_test_pca, pca\n\n# Apply PCA with different thresholds\nalphas = [0.8, 0.85, 0.9, 0.95] ## the information needed to keep from pca taining \npca_results = {}\n\nfor alpha in alphas:\n    X_train_pca, X_test_pca, pca = apply_pca(X_train, X_test, alpha)\n    pca_results[alpha] = {\n        'train': X_train_pca,\n        'test': X_test_pca,\n        'pca': pca,\n        'n_components': pca.n_components,  # Fixed: no underscore\n        'explained_variance': pca.explained_variance_ratio  # Fixed: no underscore\n    }\n    print(f\"Alpha: {alpha}, Components: {pca.n_components}, Explained Variance: {np.sum(pca.explained_variance_ratio):.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'numpy._core._exceptions._ArrayMemoryError'>",
          "evalue": "Unable to allocate 810. MiB for an array with shape (10304, 10304) and data type float64",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m pca_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alphas:\n\u001b[0;32m---> 17\u001b[0m     X_train_pca, X_test_pca, pca \u001b[38;5;241m=\u001b[39m \u001b[43mapply_pca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     pca_results[alpha] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: X_train_pca,\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: X_test_pca,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplained_variance\u001b[39m\u001b[38;5;124m'\u001b[39m: pca\u001b[38;5;241m.\u001b[39mexplained_variance_ratio  \u001b[38;5;66;03m# Fixed: no underscore\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     }\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlpha: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Components: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpca\u001b[38;5;241m.\u001b[39mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Explained Variance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(pca\u001b[38;5;241m.\u001b[39mexplained_variance_ratio)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[17], line 8\u001b[0m, in \u001b[0;36mapply_pca\u001b[0;34m(X_train, X_test, alpha)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mmake a pojection for x_test  not need to fit but train must fit then make a projuction\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mmake a fit transform for x_train \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(alpha\u001b[38;5;241m=\u001b[39malpha)  \n\u001b[0;32m----> 8\u001b[0m X_train_pca \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m      9\u001b[0m X_test_pca \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(X_test) \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train_pca, X_test_pca, pca\n",
            "Cell \u001b[0;32mIn[16], line 55\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "Cell \u001b[0;32mIn[16], line 28\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     26\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(eigenvalues)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvalues \u001b[38;5;241m=\u001b[39m eigenvalues[sorted_indices]\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvectors \u001b[38;5;241m=\u001b[39m \u001b[43meigenvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meigenvalues.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvalues)\n\u001b[1;32m     31\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meigenvectors.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvectors)\n",
            "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 810. MiB for an array with shape (10304, 10304) and data type float64"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 17
    },
    {
      "id": "c9ed300e-24f9-4f3f-a5d2-ab8f21eb10e8",
      "cell_type": "code",
      "source": "from scipy.stats import multivariate_normal\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom scipy.special import logsumexp\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\n\nclass GMM:\n    def __init__(self, K):\n        self.K = K\n\n    def initialize_params(self):\n        self.r = np.full((self.N, self.K), 1 / self.K)\n        self.pi = np.full(self.K, 1 / self.K)\n\n        kmeans = KMeans(n_clusters=self.K, n_init=10, random_state=0).fit(self.data)\n        self.mean = kmeans.cluster_centers_\n\n        self.cov = np.array([np.eye(self.M) for _ in range(self.K)])\n\n    def expectation(self):\n        log_r = np.zeros((self.N, self.K))\n\n        for k in range(self.K):\n            rv = multivariate_normal(mean=self.mean[k], cov=self.cov[k], allow_singular=True)\n            log_r[:, k] = np.log(self.pi[k] + 1e-10) + rv.logpdf(self.data)\n\n       \n        logsumexp_r = logsumexp(log_r, axis=1, keepdims=True)\n        self.r = np.exp(log_r - logsumexp_r)\n\n    def update_mixing(self):\n        self.pi = np.sum(self.r, axis=0) / self.N\n\n    def update_mean(self):\n        for k in range(self.K):\n            weight_sum = np.sum(self.r[:, k])\n            self.mean[k] = np.sum(self.r[:, k][:, None] * self.data, axis=0) / weight_sum\n\n    def update_cov(self):\n        for k in range(self.K):\n            cov_k = np.zeros((self.M, self.M))\n            for n in range(self.N):\n                diff = self.data[n] - self.mean[k]\n                cov_k += self.r[n, k] * np.outer(diff, diff)\n            self.cov[k] = cov_k / np.sum(self.r[:, k]) + 1e-6 * np.eye(self.M)\n\n    def maximization(self):\n        self.update_mixing()\n        self.update_mean()\n        self.update_cov()\n\n    def log_likelihood(self):\n        log_r = np.zeros((self.N, self.K))\n        for k in range(self.K):\n            rv = multivariate_normal(mean=self.mean[k], cov=self.cov[k], allow_singular=True)\n            log_r[:, k] = np.log(self.pi[k] + 1e-10) + rv.logpdf(self.data)\n        return np.sum(logsumexp(log_r, axis=1))\n\n    def fit(self, data, max_iters=100, tol=1e-5):\n        self.data = data\n        self.N, self.M = data.shape\n        self.initialize_params()\n\n        prev_log_likelihood = None\n        for iteration in range(max_iters):\n            self.expectation()\n            self.maximization()\n\n            log_likelihood = self.log_likelihood()\n\n            if prev_log_likelihood is not None and abs(log_likelihood - prev_log_likelihood) < tol:\n                break\n            prev_log_likelihood = log_likelihood\n\n    def predict_proba(self, x):\n        log_probs = np.array([\n            np.log(self.pi[k] + 1e-10) + multivariate_normal(mean=self.mean[k], cov=self.cov[k], allow_singular=True).logpdf(x)\n            for k in range(self.K)\n        ])\n        log_probs -= logsumexp(log_probs)\n        return np.exp(log_probs)\n\n    def predict(self, test_data):\n        return np.array([np.argmax(self.predict_proba(x)) for x in test_data])\n\n    def assign_clusters_to_classes(self, y_train):\n        self.cluster_label_map = {}\n        for k in range(self.K):\n            scores = np.zeros(40)\n            for n in range(self.N):\n                class_label = y_train[n]\n                scores[class_label - 1] += self.r[n, k]\n            self.cluster_label_map[k] = np.argmax(scores) + 1\n\n    def evaluate_accuracy(self, X_test, y_test):\n        predicted_clusters = g.predict(X_test)\n        y_pred = np.array([g.cluster_label_map.get(c, -1) for c in predicted_clusters])\n        \n        accuracy = accuracy_score(y_test, y_pred)\n\n        f1 = f1_score(y_test, y_pred, average='macro')\n\n        cm = confusion_matrix(y_test, y_pred)\n\n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"F1-score : {f1:.4f}\")\n        print(\"Confusion matrix:\")\n        print(cm)\n        return accuracy, f1, cm\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "id": "6f0b179c-e8c1-4b32-933a-225892744f06",
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\n\nalphas =[0.8, 0.85, 0.9, 0.95]\nK = [20, 40, 60]\n\nfor k in K:\n    for alpha in alphas:\n        # scaler = StandardScaler()\n        # X_train_scaled = scaler.fit_transform(X_train)\n        # X_test_scaled = scaler.transform(X_test)\n        \n        pca = PCA(alpha=alpha)\n        X_train_pca = pca.fit_transform(X_train)\n        X_test_pca = pca.transform(X_test)\n\n        g = GMM(k)\n        g.fit(X_train_pca)\n        g.assign_clusters_to_classes(y_train)\n        \n        print(f\"Gmm accuracy with k = {k} and alpha = {alpha}: \")\n        g.evaluate_accuracy(X_test_pca, y_test)\n        ",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'EOFError'>",
          "evalue": "No data left in file",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alphas:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# scaler = StandardScaler()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# X_train_scaled = scaler.fit_transform(X_train)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# X_test_scaled = scaler.transform(X_test)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     pca \u001b[38;5;241m=\u001b[39m PCA(alpha\u001b[38;5;241m=\u001b[39malpha)\n\u001b[0;32m---> 13\u001b[0m     X_train_pca \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     X_test_pca \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m     16\u001b[0m     g \u001b[38;5;241m=\u001b[39m GMM(k)\n",
            "Cell \u001b[0;32mIn[26], line 55\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "Cell \u001b[0;32mIn[26], line 22\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meigenvalues.npy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meigenvectors.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meigenvalues.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvectors \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meigenvectors.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     cov_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(X_centered, rowvar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:464\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    462\u001b[0m magic \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread(N)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[1;32m    467\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n",
            "\u001b[0;31mEOFError\u001b[0m: No data left in file"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 29
    },
    {
      "id": "099de5e4-9af2-490f-9671-03058060f412",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}